{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40fb73cf-9340-489b-886c-96a32f9ef05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-05 22:06:47--  https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv\n",
      "Resolving www.ncei.noaa.gov (www.ncei.noaa.gov)... 205.167.25.167, 205.167.25.178, 205.167.25.168, ...\n",
      "Connecting to www.ncei.noaa.gov (www.ncei.noaa.gov)|205.167.25.167|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2907119 (2.8M) [text/csv]\n",
      "Saving to: ‘data/isd-history.csv’\n",
      "\n",
      "data/isd-history.cs 100%[===================>]   2.77M  3.62MB/s    in 0.8s    \n",
      "\n",
      "2025-05-05 22:06:48 (3.62 MB/s) - ‘data/isd-history.csv’ saved [2907119/2907119]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv -O data/isd-history.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8594e1e3-d402-4341-a9a4-098da48b73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9771b33-d244-4bc8-a94b-5c3a8965f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Processing year: 1970\n",
      "🔍 Found 2757 CSV files for 1970\n",
      "📥 Year 1970 complete: 0/2757 files downloaded\n",
      "\n",
      "📅 Processing year: 1971\n",
      "🔍 Found 2491 CSV files for 1971\n",
      "📥 Year 1971 complete: 0/2491 files downloaded\n",
      "\n",
      "📅 Processing year: 1972\n",
      "🔍 Found 587 CSV files for 1972\n",
      "📥 Year 1972 complete: 0/587 files downloaded\n",
      "\n",
      "📅 Processing year: 1973\n",
      "🔍 Found 7996 CSV files for 1973\n",
      "📥 Year 1973 complete: 0/7996 files downloaded\n",
      "\n",
      "📅 Processing year: 1974\n",
      "🔍 Found 8191 CSV files for 1974\n",
      "📥 Year 1974 complete: 0/8191 files downloaded\n",
      "\n",
      "📅 Processing year: 1975\n",
      "🔍 Found 8425 CSV files for 1975\n",
      "📥 Year 1975 complete: 0/8425 files downloaded\n",
      "\n",
      "📅 Processing year: 1976\n",
      "🔍 Found 8387 CSV files for 1976\n",
      "📥 Year 1976 complete: 0/8387 files downloaded\n",
      "\n",
      "📅 Processing year: 1977\n",
      "🔍 Found 8940 CSV files for 1977\n",
      "📥 Year 1977 complete: 0/8940 files downloaded\n",
      "\n",
      "📅 Processing year: 1978\n",
      "🔍 Found 8442 CSV files for 1978\n",
      "📥 Year 1978 complete: 0/8442 files downloaded\n",
      "\n",
      "📅 Processing year: 1979\n",
      "🔍 Found 8550 CSV files for 1979\n",
      "📥 Year 1979 complete: 0/8550 files downloaded\n",
      "\n",
      "📅 Processing year: 1980\n",
      "🔍 Found 8511 CSV files for 1980\n",
      "📥 Year 1980 complete: 0/8511 files downloaded\n",
      "\n",
      "📅 Processing year: 1981\n",
      "🔍 Found 8572 CSV files for 1981\n",
      "📥 Year 1981 complete: 0/8572 files downloaded\n",
      "\n",
      "📅 Processing year: 1982\n",
      "🔍 Found 8446 CSV files for 1982\n",
      "📥 Year 1982 complete: 0/8446 files downloaded\n",
      "\n",
      "📅 Processing year: 1983\n",
      "🔍 Found 8512 CSV files for 1983\n",
      "📥 Year 1983 complete: 0/8512 files downloaded\n",
      "\n",
      "📅 Processing year: 1984\n",
      "🔍 Found 8677 CSV files for 1984\n",
      "📥 Year 1984 complete: 0/8677 files downloaded\n",
      "\n",
      "📅 Processing year: 1985\n",
      "🔍 Found 8925 CSV files for 1985\n",
      "📥 Year 1985 complete: 1/8925 files downloaded\n",
      "\n",
      "📅 Processing year: 1986\n",
      "🔍 Found 8891 CSV files for 1986\n",
      "📥 Year 1986 complete: 1864/8891 files downloaded\n",
      "\n",
      "📅 Processing year: 1987\n",
      "🔍 Found 9030 CSV files for 1987\n",
      "✅ Downloaded 2000 files in 180 seconds\n",
      "⏳ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/1987/03212099999.csv – retrying in 1 sec...\n",
      "⏳ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/1987/03658099999.csv – retrying in 1 sec...\n",
      "✅ Downloaded 4000 files in 275 seconds\n",
      "✅ Downloaded 6000 files in 370 seconds\n",
      "✅ Downloaded 8000 files in 457 seconds\n",
      "✅ Downloaded 10000 files in 546 seconds\n",
      "📥 Year 1987 complete: 9030/9030 files downloaded\n",
      "\n",
      "📅 Processing year: 1988\n",
      "🔍 Found 9175 CSV files for 1988\n",
      "✅ Downloaded 12000 files in 641 seconds\n",
      "✅ Downloaded 14000 files in 730 seconds\n",
      "✅ Downloaded 16000 files in 817 seconds\n",
      "✅ Downloaded 18000 files in 905 seconds\n",
      "✅ Downloaded 20000 files in 990 seconds\n",
      "📥 Year 1988 complete: 9175/9175 files downloaded\n",
      "\n",
      "📅 Processing year: 1989\n",
      "🔍 Found 9252 CSV files for 1989\n",
      "✅ Downloaded 22000 files in 1087 seconds\n",
      "✅ Downloaded 24000 files in 1174 seconds\n",
      "⏳ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/1989/44237099999.csv – retrying in 1 sec...\n",
      "✅ Downloaded 26000 files in 1259 seconds\n",
      "✅ Downloaded 28000 files in 1343 seconds\n",
      "📥 Year 1989 complete: 9252/9252 files downloaded\n",
      "\n",
      "📅 Processing year: 1990\n",
      "🔍 Found 9712 CSV files for 1990\n",
      "✅ Downloaded 30000 files in 1433 seconds\n",
      "✅ Downloaded 32000 files in 1519 seconds\n",
      "✅ Downloaded 34000 files in 1605 seconds\n",
      "✅ Downloaded 36000 files in 1685 seconds\n",
      "✅ Downloaded 38000 files in 1772 seconds\n",
      "📥 Year 1990 complete: 9712/9712 files downloaded\n",
      "\n",
      "📅 Processing year: 1991\n",
      "🔍 Found 9691 CSV files for 1991\n",
      "✅ Downloaded 40000 files in 1864 seconds\n",
      "✅ Downloaded 42000 files in 1951 seconds\n",
      "✅ Downloaded 44000 files in 2040 seconds\n",
      "✅ Downloaded 46000 files in 2122 seconds\n",
      "✅ Downloaded 48000 files in 2209 seconds\n",
      "📥 Year 1991 complete: 9691/9691 files downloaded\n",
      "\n",
      "📅 Processing year: 1992\n",
      "🔍 Found 9140 CSV files for 1992\n",
      "✅ Downloaded 50000 files in 2300 seconds\n",
      "✅ Downloaded 52000 files in 2385 seconds\n",
      "✅ Downloaded 54000 files in 2470 seconds\n",
      "✅ Downloaded 56000 files in 2553 seconds\n",
      "📥 Year 1992 complete: 9140/9140 files downloaded\n",
      "\n",
      "📅 Processing year: 1993\n",
      "🔍 Found 9006 CSV files for 1993\n",
      "✅ Downloaded 58000 files in 2641 seconds\n",
      "✅ Downloaded 60000 files in 2731 seconds\n",
      "✅ Downloaded 62000 files in 2818 seconds\n",
      "✅ Downloaded 64000 files in 2906 seconds\n",
      "✅ Downloaded 66000 files in 2997 seconds\n",
      "📥 Year 1993 complete: 9006/9006 files downloaded\n",
      "\n",
      "📅 Processing year: 1994\n",
      "🔍 Found 8952 CSV files for 1994\n",
      "✅ Downloaded 68000 files in 3095 seconds\n",
      "✅ Downloaded 70000 files in 3187 seconds\n",
      "✅ Downloaded 72000 files in 3275 seconds\n",
      "✅ Downloaded 74000 files in 3361 seconds\n",
      "📥 Year 1994 complete: 8952/8952 files downloaded\n",
      "\n",
      "📅 Processing year: 1995\n",
      "🔍 Found 8790 CSV files for 1995\n",
      "✅ Downloaded 76000 files in 3453 seconds\n",
      "✅ Downloaded 78000 files in 3542 seconds\n",
      "✅ Downloaded 80000 files in 3628 seconds\n",
      "✅ Downloaded 82000 files in 3710 seconds\n",
      "✅ Downloaded 84000 files in 3797 seconds\n",
      "📥 Year 1995 complete: 8790/8790 files downloaded\n",
      "\n",
      "📅 Processing year: 1996\n",
      "🔍 Found 8597 CSV files for 1996\n",
      "✅ Downloaded 86000 files in 3895 seconds\n",
      "✅ Downloaded 88000 files in 3984 seconds\n",
      "✅ Downloaded 90000 files in 4072 seconds\n",
      "✅ Downloaded 92000 files in 4160 seconds\n",
      "📥 Year 1996 complete: 8597/8597 files downloaded\n",
      "\n",
      "📅 Processing year: 1997\n",
      "🔍 Found 8540 CSV files for 1997\n",
      "✅ Downloaded 94000 files in 4254 seconds\n",
      "✅ Downloaded 96000 files in 4346 seconds\n",
      "✅ Downloaded 98000 files in 4433 seconds\n",
      "✅ Downloaded 100000 files in 4525 seconds\n",
      "📥 Year 1997 complete: 8540/8540 files downloaded\n",
      "\n",
      "📅 Processing year: 1998\n",
      "🔍 Found 8519 CSV files for 1998\n",
      "✅ Downloaded 102000 files in 4620 seconds\n",
      "✅ Downloaded 104000 files in 4712 seconds\n",
      "✅ Downloaded 106000 files in 4800 seconds\n",
      "⏳ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/1998/71914399999.csv – retrying in 1 sec...\n",
      "✅ Downloaded 108000 files in 4889 seconds\n",
      "✅ Downloaded 110000 files in 4980 seconds\n",
      "📥 Year 1998 complete: 8519/8519 files downloaded\n",
      "\n",
      "📅 Processing year: 1999\n",
      "🔍 Found 8464 CSV files for 1999\n",
      "✅ Downloaded 112000 files in 5076 seconds\n",
      "✅ Downloaded 114000 files in 5169 seconds\n",
      "✅ Downloaded 116000 files in 5255 seconds\n",
      "✅ Downloaded 118000 files in 5344 seconds\n",
      "📥 Year 1999 complete: 8464/8464 files downloaded\n",
      "\n",
      "📅 Processing year: 2000\n",
      "🔍 Found 8279 CSV files for 2000\n",
      "⏳ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2000/03520099999.csv – retrying in 1 sec...\n",
      "✅ Downloaded 120000 files in 5442 seconds\n",
      "✅ Downloaded 122000 files in 5534 seconds\n",
      "✅ Downloaded 124000 files in 5622 seconds\n",
      "✅ Downloaded 126000 files in 5717 seconds\n",
      "📥 Year 2000 complete: 8279/8279 files downloaded\n",
      "\n",
      "📅 Processing year: 2001\n",
      "🔍 Found 9008 CSV files for 2001\n",
      "✅ Downloaded 128000 files in 5813 seconds\n",
      "✅ Downloaded 130000 files in 5903 seconds\n",
      "✅ Downloaded 132000 files in 5992 seconds\n",
      "✅ Downloaded 134000 files in 6082 seconds\n",
      "✅ Downloaded 136000 files in 6170 seconds\n",
      "📥 Year 2001 complete: 9008/9008 files downloaded\n",
      "\n",
      "📅 Processing year: 2002\n",
      "🔍 Found 8990 CSV files for 2002\n",
      "✅ Downloaded 138000 files in 6267 seconds\n",
      "✅ Downloaded 140000 files in 6360 seconds\n",
      "✅ Downloaded 142000 files in 6454 seconds\n",
      "✅ Downloaded 144000 files in 6549 seconds\n",
      "📥 Year 2002 complete: 8990/8990 files downloaded\n",
      "\n",
      "📅 Processing year: 2003\n",
      "🔍 Found 9081 CSV files for 2003\n",
      "✅ Downloaded 146000 files in 6645 seconds\n",
      "✅ Downloaded 148000 files in 6736 seconds\n",
      "✅ Downloaded 150000 files in 6823 seconds\n",
      "✅ Downloaded 152000 files in 6917 seconds\n",
      "✅ Downloaded 154000 files in 7005 seconds\n",
      "📥 Year 2003 complete: 9081/9081 files downloaded\n",
      "\n",
      "📅 Processing year: 2004\n",
      "🔍 Found 9574 CSV files for 2004\n",
      "✅ Downloaded 156000 files in 7104 seconds\n",
      "✅ Downloaded 158000 files in 7197 seconds\n",
      "✅ Downloaded 160000 files in 7285 seconds\n",
      "✅ Downloaded 162000 files in 7377 seconds\n",
      "📥 Year 2004 complete: 9574/9574 files downloaded\n",
      "\n",
      "📅 Processing year: 2005\n",
      "🔍 Found 10130 CSV files for 2005\n",
      "✅ Downloaded 164000 files in 7478 seconds\n",
      "✅ Downloaded 166000 files in 7569 seconds\n",
      "✅ Downloaded 168000 files in 7657 seconds\n",
      "✅ Downloaded 170000 files in 7749 seconds\n",
      "✅ Downloaded 172000 files in 7843 seconds\n",
      "📥 Year 2005 complete: 10130/10130 files downloaded\n",
      "\n",
      "📅 Processing year: 2006\n",
      "🔍 Found 9479 CSV files for 2006\n",
      "✅ Downloaded 174000 files in 7945 seconds\n",
      "✅ Downloaded 176000 files in 8042 seconds\n",
      "✅ Downloaded 178000 files in 8139 seconds\n",
      "✅ Downloaded 180000 files in 8228 seconds\n",
      "✅ Downloaded 182000 files in 8322 seconds\n",
      "📥 Year 2006 complete: 9479/9479 files downloaded\n",
      "\n",
      "📅 Processing year: 2007\n",
      "🔍 Found 9782 CSV files for 2007\n",
      "✅ Downloaded 184000 files in 8419 seconds\n",
      "✅ Downloaded 186000 files in 8508 seconds\n",
      "✅ Downloaded 188000 files in 8599 seconds\n",
      "⏳ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2007/71026899999.csv – retrying in 1 sec...\n",
      "✅ Downloaded 190000 files in 8688 seconds\n",
      "⏳ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2007/72434503966.csv – retrying in 1 sec...\n",
      "✅ Downloaded 192000 files in 8780 seconds\n",
      "📥 Year 2007 complete: 9782/9782 files downloaded\n",
      "\n",
      "📅 Processing year: 2008\n",
      "🔍 Found 10363 CSV files for 2008\n",
      "✅ Downloaded 194000 files in 8880 seconds\n",
      "✅ Downloaded 196000 files in 8973 seconds\n",
      "✅ Downloaded 198000 files in 9067 seconds\n",
      "✅ Downloaded 200000 files in 9162 seconds\n",
      "✅ Downloaded 202000 files in 9257 seconds\n",
      "📥 Year 2008 complete: 10363/10363 files downloaded\n",
      "\n",
      "📅 Processing year: 2009\n",
      "🔍 Found 10723 CSV files for 2009\n",
      "✅ Downloaded 204000 files in 9361 seconds\n",
      "✅ Downloaded 206000 files in 9455 seconds\n",
      "✅ Downloaded 208000 files in 9549 seconds\n",
      "✅ Downloaded 210000 files in 9641 seconds\n",
      "⏳ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2009/72635594871.csv – retrying in 1 sec...\n",
      "✅ Downloaded 212000 files in 9736 seconds\n",
      "✅ Downloaded 214000 files in 9827 seconds\n",
      "📥 Year 2009 complete: 10723/10723 files downloaded\n",
      "\n",
      "📅 Processing year: 2010\n",
      "🔍 Found 10902 CSV files for 2010\n",
      "✅ Downloaded 216000 files in 9927 seconds\n",
      "✅ Downloaded 218000 files in 10018 seconds\n",
      "✅ Downloaded 220000 files in 10111 seconds\n",
      "✅ Downloaded 222000 files in 10205 seconds\n",
      "✅ Downloaded 224000 files in 10295 seconds\n",
      "📥 Year 2010 complete: 10902/10902 files downloaded\n",
      "\n",
      "📅 Processing year: 2011\n",
      "🔍 Found 11088 CSV files for 2011\n",
      "✅ Downloaded 226000 files in 10393 seconds\n",
      "✅ Downloaded 228000 files in 10488 seconds\n",
      "✅ Downloaded 230000 files in 10586 seconds\n",
      "✅ Downloaded 232000 files in 10682 seconds\n",
      "✅ Downloaded 234000 files in 10776 seconds\n",
      "✅ Downloaded 236000 files in 10867 seconds\n",
      "📥 Year 2011 complete: 11088/11088 files downloaded\n",
      "\n",
      "📅 Processing year: 2012\n",
      "🔍 Found 11875 CSV files for 2012\n",
      "✅ Downloaded 238000 files in 10964 seconds\n",
      "✅ Downloaded 240000 files in 11051 seconds\n",
      "✅ Downloaded 242000 files in 11143 seconds\n",
      "✅ Downloaded 244000 files in 11235 seconds\n",
      "✅ Downloaded 246000 files in 11329 seconds\n",
      "✅ Downloaded 248000 files in 11418 seconds\n",
      "📥 Year 2012 complete: 11875/11875 files downloaded\n",
      "\n",
      "📅 Processing year: 2013\n",
      "🔍 Found 11870 CSV files for 2013\n",
      "✅ Downloaded 250000 files in 11518 seconds\n",
      "✅ Downloaded 252000 files in 11609 seconds\n",
      "✅ Downloaded 254000 files in 11698 seconds\n",
      "✅ Downloaded 256000 files in 11789 seconds\n",
      "✅ Downloaded 258000 files in 11883 seconds\n",
      "📥 Year 2013 complete: 11870/11870 files downloaded\n",
      "\n",
      "📅 Processing year: 2014\n",
      "🔍 Found 11958 CSV files for 2014\n",
      "✅ Downloaded 260000 files in 11981 seconds\n",
      "✅ Downloaded 262000 files in 12081 seconds\n",
      "✅ Downloaded 264000 files in 12175 seconds\n",
      "✅ Downloaded 266000 files in 12273 seconds\n",
      "✅ Downloaded 268000 files in 12366 seconds\n",
      "✅ Downloaded 270000 files in 12458 seconds\n",
      "📥 Year 2014 complete: 11958/11958 files downloaded\n",
      "\n",
      "📅 Processing year: 2015\n",
      "🔍 Found 12100 CSV files for 2015\n",
      "✅ Downloaded 272000 files in 12553 seconds\n",
      "✅ Downloaded 274000 files in 12647 seconds\n",
      "✅ Downloaded 276000 files in 12744 seconds\n",
      "✅ Downloaded 278000 files in 12838 seconds\n",
      "✅ Downloaded 280000 files in 12931 seconds\n",
      "✅ Downloaded 282000 files in 13027 seconds\n",
      "📥 Year 2015 complete: 12100/12100 files downloaded\n",
      "\n",
      "📅 Processing year: 2016\n",
      "🔍 Found 12111 CSV files for 2016\n",
      "✅ Downloaded 284000 files in 13125 seconds\n",
      "✅ Downloaded 286000 files in 13218 seconds\n",
      "✅ Downloaded 288000 files in 13310 seconds\n",
      "✅ Downloaded 290000 files in 13404 seconds\n",
      "✅ Downloaded 292000 files in 13497 seconds\n",
      "✅ Downloaded 294000 files in 13594 seconds\n",
      "✅ Downloaded 296000 files in 13683 seconds\n",
      "📥 Year 2016 complete: 12111/12111 files downloaded\n",
      "\n",
      "📅 Processing year: 2017\n",
      "🔍 Found 12337 CSV files for 2017\n",
      "✅ Downloaded 298000 files in 13783 seconds\n",
      "✅ Downloaded 300000 files in 13876 seconds\n",
      "✅ Downloaded 302000 files in 13967 seconds\n",
      "✅ Downloaded 304000 files in 14062 seconds\n",
      "✅ Downloaded 306000 files in 14162 seconds\n",
      "✅ Downloaded 308000 files in 14257 seconds\n",
      "📥 Year 2017 complete: 12337/12337 files downloaded\n",
      "\n",
      "📅 Processing year: 2018\n",
      "🔍 Found 12426 CSV files for 2018\n",
      "✅ Downloaded 310000 files in 14354 seconds\n",
      "✅ Downloaded 312000 files in 14449 seconds\n",
      "✅ Downloaded 314000 files in 14548 seconds\n",
      "✅ Downloaded 316000 files in 14644 seconds\n",
      "✅ Downloaded 318000 files in 14739 seconds\n",
      "✅ Downloaded 320000 files in 14834 seconds\n",
      "📥 Year 2018 complete: 12426/12426 files downloaded\n",
      "\n",
      "📅 Processing year: 2019\n",
      "🔍 Found 12387 CSV files for 2019\n",
      "✅ Downloaded 322000 files in 14933 seconds\n",
      "✅ Downloaded 324000 files in 15024 seconds\n",
      "✅ Downloaded 326000 files in 15114 seconds\n",
      "✅ Downloaded 328000 files in 15205 seconds\n",
      "✅ Downloaded 330000 files in 15298 seconds\n",
      "✅ Downloaded 332000 files in 15386 seconds\n",
      "📥 Year 2019 complete: 12387/12387 files downloaded\n",
      "\n",
      "📅 Processing year: 2020\n",
      "🔍 Found 12299 CSV files for 2020\n",
      "✅ Downloaded 334000 files in 15489 seconds\n",
      "✅ Downloaded 336000 files in 15586 seconds\n",
      "✅ Downloaded 338000 files in 15675 seconds\n",
      "✅ Downloaded 340000 files in 15765 seconds\n",
      "✅ Downloaded 342000 files in 15858 seconds\n",
      "✅ Downloaded 344000 files in 15947 seconds\n",
      "📥 Year 2020 complete: 12299/12299 files downloaded\n",
      "\n",
      "📅 Processing year: 2021\n",
      "🔍 Found 12275 CSV files for 2021\n",
      "✅ Downloaded 346000 files in 16047 seconds\n",
      "✅ Downloaded 348000 files in 16146 seconds\n",
      "✅ Downloaded 350000 files in 16240 seconds\n",
      "✅ Downloaded 352000 files in 16343 seconds\n",
      "✅ Downloaded 354000 files in 16436 seconds\n",
      "✅ Downloaded 356000 files in 16528 seconds\n",
      "📥 Year 2021 complete: 12275/12275 files downloaded\n",
      "\n",
      "📅 Processing year: 2022\n",
      "🔍 Found 12319 CSV files for 2022\n",
      "✅ Downloaded 358000 files in 16629 seconds\n",
      "✅ Downloaded 360000 files in 16719 seconds\n",
      "✅ Downloaded 362000 files in 16812 seconds\n",
      "✅ Downloaded 364000 files in 16909 seconds\n",
      "✅ Downloaded 366000 files in 17004 seconds\n",
      "✅ Downloaded 368000 files in 17103 seconds\n",
      "✅ Downloaded 370000 files in 17191 seconds\n",
      "📥 Year 2022 complete: 12319/12319 files downloaded\n",
      "\n",
      "📅 Processing year: 2023\n",
      "🔍 Found 12311 CSV files for 2023\n",
      "✅ Downloaded 372000 files in 17287 seconds\n",
      "✅ Downloaded 374000 files in 17375 seconds\n",
      "✅ Downloaded 376000 files in 17463 seconds\n",
      "✅ Downloaded 378000 files in 17550 seconds\n",
      "✅ Downloaded 380000 files in 17641 seconds\n",
      "✅ Downloaded 382000 files in 17736 seconds\n",
      "📥 Year 2023 complete: 12311/12311 files downloaded\n",
      "\n",
      "✅ All done! Total downloaded: 382400 files in 17754 seconds.\n"
     ]
    }
   ],
   "source": [
    "## Script to download all NOAA weather data from 1970 - 2023\n",
    "\n",
    "\n",
    "NOAA_BASE_URL = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access\"\n",
    "BASE_PATH = \"/home/jovyan/Project/gsod\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "YEARS = list(range(1970, 2024))\n",
    "MAX_THREADS = 10  \n",
    "\n",
    "os.makedirs(BASE_PATH, exist_ok=True)\n",
    "\n",
    "def download_file(file_url, local_path, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            if not os.path.exists(local_path):\n",
    "                urllib.request.urlretrieve(file_url, local_path)\n",
    "                return True\n",
    "            else:\n",
    "                return False  # Already exists\n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.code == 503:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"⏳ 503 for {file_url} – retrying in {wait_time} sec...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"❌ HTTP error for {file_url}: {e}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Other error for {file_url}: {e}\")\n",
    "            break\n",
    "    return False\n",
    "\n",
    "start_time = time.time()\n",
    "total_downloaded = 0\n",
    "failed_downloads = []\n",
    "\n",
    "try:\n",
    "    for year in YEARS:\n",
    "        print(f\"\\n📅 Processing year: {year}\")\n",
    "        year_path = os.path.join(BASE_PATH, str(year))\n",
    "        os.makedirs(year_path, exist_ok=True)\n",
    "\n",
    "        url = f\"{NOAA_BASE_URL}/{year}/\"\n",
    "        req = urllib.request.Request(url, headers=HEADERS)\n",
    "\n",
    "        try:\n",
    "            with urllib.request.urlopen(req) as response:\n",
    "                html = response.read().decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to fetch directory listing for {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        csv_links = [link.get(\"href\") for link in soup.find_all(\"a\") if link.get(\"href\", \"\").endswith(\".csv\")]\n",
    "        print(f\"🔍 Found {len(csv_links)} CSV files for {year}\")\n",
    "\n",
    "        tasks = []\n",
    "        with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "            for filename in csv_links:\n",
    "                file_url = url + filename\n",
    "                local_file = os.path.join(year_path, filename)\n",
    "                tasks.append(executor.submit(download_file, file_url, local_file))\n",
    "\n",
    "            year_downloaded = 0\n",
    "            for future, filename in zip(as_completed(tasks), csv_links):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        total_downloaded += 1\n",
    "                        year_downloaded += 1\n",
    "                        if total_downloaded % 2000 == 0:\n",
    "                            elapsed = int(time.time() - start_time)\n",
    "                            print(f\"✅ Downloaded {total_downloaded} files in {elapsed} seconds\")\n",
    "                except Exception as e:\n",
    "                    failed_downloads.append(url + filename)\n",
    "                    print(f\"❌ Failed to download {filename}: {e}\")\n",
    "\n",
    "        print(f\"📥 Year {year} complete: {year_downloaded}/{len(csv_links)} files downloaded\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️ Interrupted by user\")\n",
    "\n",
    "#  Save failed downloads\n",
    "if failed_downloads:\n",
    "    with open(\"failed_downloads.txt\", \"w\") as f:\n",
    "        for url in failed_downloads:\n",
    "            f.write(url + \"\\n\")\n",
    "    print(f\"⚠️ Logged {len(failed_downloads)} failed downloads to 'failed_downloads.txt'\")\n",
    "\n",
    "#  Done\n",
    "elapsed = int(time.time() - start_time)\n",
    "print(f\"\\n✅ All done! Total downloaded: {total_downloaded} files in {elapsed} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c74c00-b3e7-4a8b-ab5f-afc66f35118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CSV folder size: 32.70 GB\n"
     ]
    }
   ],
   "source": [
    "## Verify size of download\n",
    "\n",
    "def get_folder_size(path):\n",
    "    total = 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if os.path.isfile(fp):\n",
    "                total += os.path.getsize(fp)\n",
    "    return total / (1024 ** 3)  \n",
    "\n",
    "size_gb = get_folder_size(\"/home/jovyan/Project/gsod\")\n",
    "print(f\"Original CSV folder size: {size_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c411e7-bce5-4f24-bdc3-5dbccd10ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GSOD CSV to Parquet\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.range(15).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2eb80c-0078-4d59-91f7-30009e1675af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: CSVs converted to Parquet\n"
     ]
    }
   ],
   "source": [
    "## Convert to Parquet\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import substring, lpad, concat_ws, col\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", DoubleType(), True),\n",
    "    StructField(\"LONGITUDE\", DoubleType(), True),\n",
    "    StructField(\"ELEVATION\", DoubleType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"TEMP\", DoubleType(), True),\n",
    "    StructField(\"DEWP\", DoubleType(), True),\n",
    "    StructField(\"SLP\", DoubleType(), True),\n",
    "    StructField(\"STP\", DoubleType(), True),\n",
    "    StructField(\"VISIB\", DoubleType(), True),\n",
    "    StructField(\"WDSP\", DoubleType(), True),\n",
    "    StructField(\"MXSPD\", DoubleType(), True),\n",
    "    StructField(\"GUST\", DoubleType(), True),\n",
    "    StructField(\"MAX\", StringType(), True),\n",
    "    StructField(\"MIN\", StringType(), True),\n",
    "    StructField(\"PRCP\", StringType(), True),\n",
    "    StructField(\"SNDP\", DoubleType(), True),\n",
    "    StructField(\"FRSHTT\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"/home/jovyan/Project/gsod\")\n",
    "\n",
    "df = df.withColumn(\"year\", substring(\"DATE\", 1, 4))\n",
    "\n",
    "#  Load station metadata\n",
    "meta = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"data/isd-history.csv\")\n",
    "\n",
    "# Create STATION ID to match GSOD format\n",
    "meta = meta.withColumn(\"STATION\", concat_ws(\"\",\n",
    "    lpad(col(\"USAF\").cast(\"string\"), 6, \"0\"),\n",
    "    lpad(col(\"WBAN\").cast(\"string\"), 5, \"0\")\n",
    ")).withColumnRenamed(\"ELEV(M)\", \"ELEV\") \\\n",
    "  .select(\"STATION\", \"CTRY\", \"STATE\", \"LAT\", \"LON\", \"ELEV\")\n",
    "\n",
    "\n",
    "#  Join metadata into GSOD DataFrame\n",
    "df_enriched = df.join(meta, on=\"STATION\", how=\"left\")\n",
    "\n",
    "# Write to Parquet (with metadata, partitioned by year)\n",
    "df_enriched.repartition(\"year\").write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .parquet(\"/home/jovyan/Project/gsod_parquet_enriched\")\n",
    "\n",
    "print(\"Done: CSVs converted to Parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab5b7ff-3eeb-4b54-af02-414f7dfcaf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: string (nullable = true)\n",
      " |-- MIN: string (nullable = true)\n",
      " |-- PRCP: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: string (nullable = true)\n",
      " |-- CTRY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      " |-- ELEV: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "+-----------+----------+---------+----------+---------+--------------------+----+----+----+----+------+----+-----+----+-----+---+-----+----+------+----+-----+------+-------+-----+----+\n",
      "|    STATION|      DATE| LATITUDE| LONGITUDE|ELEVATION|                NAME|TEMP|DEWP| SLP| STP| VISIB|WDSP|MXSPD|GUST|  MAX|MIN| PRCP|SNDP|FRSHTT|CTRY|STATE|   LAT|    LON| ELEV|year|\n",
      "+-----------+----------+---------+----------+---------+--------------------+----+----+----+----+------+----+-----+----+-----+---+-----+----+------+----+-----+------+-------+-----+----+\n",
      "|72401599999|2023-01-01|37.074194|-77.957528|    133.8|ALLEN C PERKINSON...|55.2|24.0|51.2|24.0|9999.9| 0.0|999.9| 0.0|  8.6| 24|  5.4|24.0|  11.1|  US|   VA|37.074|-77.958|133.8|2023|\n",
      "|23847099999|2023-10-13|     61.3|      71.3|     33.4|        SYTOMINO, RS|38.3| 8.0|36.4| 8.0|1000.4| 8.0|996.3| 8.0|  6.2|  8|  9.5| 8.0|  11.7|  RS| NULL|  61.3|   71.3| 33.4|2023|\n",
      "|72401599999|2023-01-02|37.074194|-77.957528|    133.8|ALLEN C PERKINSON...|53.7|24.0|48.5|24.0|9999.9| 0.0|999.9| 0.0|  8.8| 24|  3.7|24.0|   8.9|  US|   VA|37.074|-77.958|133.8|2023|\n",
      "|23847099999|2023-10-14|     61.3|      71.3|     33.4|        SYTOMINO, RS|41.1| 8.0|37.3| 8.0| 998.0| 8.0|994.0| 8.0|  6.2|  8|  9.7| 8.0|  13.6|  RS| NULL|  61.3|   71.3| 33.4|2023|\n",
      "|72401599999|2023-01-03|37.074194|-77.957528|    133.8|ALLEN C PERKINSON...|57.8|24.0|52.3|24.0|9999.9| 0.0|999.9| 0.0|  9.4| 24|  8.9|24.0|  15.0|  US|   VA|37.074|-77.958|133.8|2023|\n",
      "+-----------+----------+---------+----------+---------+--------------------+----+----+----+----+------+----+-----+----+-----+---+-----+----+------+----+-----+------+-------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_enriched = spark.read.parquet(\"/home/jovyan/Project/gsod_parquet_enriched\")\n",
    "df_enriched.printSchema()\n",
    "df_enriched.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d1eb5c-4c21-4520-a69d-334cfe7afeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_replace, when\n",
    "# 1. Drop rows with missing essential fields\n",
    "essential_cols = [\"TEMP\", \"LATITUDE\", \"LONGITUDE\"]\n",
    "df_clean = df.dropna(subset=essential_cols)\n",
    "\n",
    "# 2. Filter out placeholder or unrealistic values\n",
    "df_clean = df_clean.filter((col(\"TEMP\") > -1750) & (col(\"TEMP\") < 1750)) \\\n",
    "                   .filter((col(\"SLP\") < 1100) & (col(\"SLP\") > 800)) \\\n",
    "                   .filter(~col(\"PRCP\").isin([\"99.99\", \"999.9\", \"99.9\", \"999.0\"]))\n",
    "\n",
    "# 3. Remove duplicates\n",
    "df_clean = df_clean.dropDuplicates([\"STATION\", \"DATE\"])\n",
    "\n",
    "# 4. Clean special flags in MAX/MIN and PRCP columns\n",
    "df_clean = df_clean.withColumn(\"MAX\", regexp_replace(col(\"MAX\"), \"[*]\", \"\").cast(\"double\"))\n",
    "df_clean = df_clean.withColumn(\"MIN\", regexp_replace(col(\"MIN\"), \"[*]\", \"\").cast(\"double\"))\n",
    "\n",
    "# 5. Remove special flags from PRCP column\n",
    "df_clean = df_clean.withColumn(\"PRCP\", regexp_replace(col(\"PRCP\"), \"[A-Z]\", \"\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e19410-bd3f-4424-8a9b-1c74e943f9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761c049-8ac5-466c-995f-511ec800bbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
