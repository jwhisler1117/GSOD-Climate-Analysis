{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40fb73cf-9340-489b-886c-96a32f9ef05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-05 22:06:47--  https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv\n",
      "Resolving www.ncei.noaa.gov (www.ncei.noaa.gov)... 205.167.25.167, 205.167.25.178, 205.167.25.168, ...\n",
      "Connecting to www.ncei.noaa.gov (www.ncei.noaa.gov)|205.167.25.167|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2907119 (2.8M) [text/csv]\n",
      "Saving to: â€˜data/isd-history.csvâ€™\n",
      "\n",
      "data/isd-history.cs 100%[===================>]   2.77M  3.62MB/s    in 0.8s    \n",
      "\n",
      "2025-05-05 22:06:48 (3.62 MB/s) - â€˜data/isd-history.csvâ€™ saved [2907119/2907119]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv -O data/isd-history.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8594e1e3-d402-4341-a9a4-098da48b73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9771b33-d244-4bc8-a94b-5c3a8965f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“… Processing year: 1970\n",
      "ðŸ” Found 2757 CSV files for 1970\n",
      "ðŸ“¥ Year 1970 complete: 0/2757 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1971\n",
      "ðŸ” Found 2491 CSV files for 1971\n",
      "ðŸ“¥ Year 1971 complete: 0/2491 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1972\n",
      "ðŸ” Found 587 CSV files for 1972\n",
      "ðŸ“¥ Year 1972 complete: 0/587 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1973\n",
      "ðŸ” Found 7996 CSV files for 1973\n",
      "ðŸ“¥ Year 1973 complete: 0/7996 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1974\n",
      "ðŸ” Found 8191 CSV files for 1974\n",
      "ðŸ“¥ Year 1974 complete: 0/8191 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1975\n",
      "ðŸ” Found 8425 CSV files for 1975\n",
      "ðŸ“¥ Year 1975 complete: 0/8425 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1976\n",
      "ðŸ” Found 8387 CSV files for 1976\n",
      "ðŸ“¥ Year 1976 complete: 0/8387 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1977\n",
      "ðŸ” Found 8940 CSV files for 1977\n",
      "ðŸ“¥ Year 1977 complete: 0/8940 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1978\n",
      "ðŸ” Found 8442 CSV files for 1978\n",
      "ðŸ“¥ Year 1978 complete: 0/8442 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1979\n",
      "ðŸ” Found 8550 CSV files for 1979\n",
      "ðŸ“¥ Year 1979 complete: 0/8550 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1980\n",
      "ðŸ” Found 8511 CSV files for 1980\n",
      "ðŸ“¥ Year 1980 complete: 0/8511 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1981\n",
      "ðŸ” Found 8572 CSV files for 1981\n",
      "ðŸ“¥ Year 1981 complete: 0/8572 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1982\n",
      "ðŸ” Found 8446 CSV files for 1982\n",
      "ðŸ“¥ Year 1982 complete: 0/8446 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1983\n",
      "ðŸ” Found 8512 CSV files for 1983\n",
      "ðŸ“¥ Year 1983 complete: 0/8512 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1984\n",
      "ðŸ” Found 8677 CSV files for 1984\n",
      "ðŸ“¥ Year 1984 complete: 0/8677 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1985\n",
      "ðŸ” Found 8925 CSV files for 1985\n",
      "ðŸ“¥ Year 1985 complete: 1/8925 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1986\n",
      "ðŸ” Found 8891 CSV files for 1986\n",
      "ðŸ“¥ Year 1986 complete: 1864/8891 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1987\n",
      "ðŸ” Found 9030 CSV files for 1987\n",
      "âœ… Downloaded 2000 files in 180 seconds\n",
      "â³ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/1987/03212099999.csv â€“ retrying in 1 sec...\n",
      "â³ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/1987/03658099999.csv â€“ retrying in 1 sec...\n",
      "âœ… Downloaded 4000 files in 275 seconds\n",
      "âœ… Downloaded 6000 files in 370 seconds\n",
      "âœ… Downloaded 8000 files in 457 seconds\n",
      "âœ… Downloaded 10000 files in 546 seconds\n",
      "ðŸ“¥ Year 1987 complete: 9030/9030 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1988\n",
      "ðŸ” Found 9175 CSV files for 1988\n",
      "âœ… Downloaded 12000 files in 641 seconds\n",
      "âœ… Downloaded 14000 files in 730 seconds\n",
      "âœ… Downloaded 16000 files in 817 seconds\n",
      "âœ… Downloaded 18000 files in 905 seconds\n",
      "âœ… Downloaded 20000 files in 990 seconds\n",
      "ðŸ“¥ Year 1988 complete: 9175/9175 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1989\n",
      "ðŸ” Found 9252 CSV files for 1989\n",
      "âœ… Downloaded 22000 files in 1087 seconds\n",
      "âœ… Downloaded 24000 files in 1174 seconds\n",
      "â³ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/1989/44237099999.csv â€“ retrying in 1 sec...\n",
      "âœ… Downloaded 26000 files in 1259 seconds\n",
      "âœ… Downloaded 28000 files in 1343 seconds\n",
      "ðŸ“¥ Year 1989 complete: 9252/9252 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1990\n",
      "ðŸ” Found 9712 CSV files for 1990\n",
      "âœ… Downloaded 30000 files in 1433 seconds\n",
      "âœ… Downloaded 32000 files in 1519 seconds\n",
      "âœ… Downloaded 34000 files in 1605 seconds\n",
      "âœ… Downloaded 36000 files in 1685 seconds\n",
      "âœ… Downloaded 38000 files in 1772 seconds\n",
      "ðŸ“¥ Year 1990 complete: 9712/9712 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1991\n",
      "ðŸ” Found 9691 CSV files for 1991\n",
      "âœ… Downloaded 40000 files in 1864 seconds\n",
      "âœ… Downloaded 42000 files in 1951 seconds\n",
      "âœ… Downloaded 44000 files in 2040 seconds\n",
      "âœ… Downloaded 46000 files in 2122 seconds\n",
      "âœ… Downloaded 48000 files in 2209 seconds\n",
      "ðŸ“¥ Year 1991 complete: 9691/9691 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1992\n",
      "ðŸ” Found 9140 CSV files for 1992\n",
      "âœ… Downloaded 50000 files in 2300 seconds\n",
      "âœ… Downloaded 52000 files in 2385 seconds\n",
      "âœ… Downloaded 54000 files in 2470 seconds\n",
      "âœ… Downloaded 56000 files in 2553 seconds\n",
      "ðŸ“¥ Year 1992 complete: 9140/9140 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1993\n",
      "ðŸ” Found 9006 CSV files for 1993\n",
      "âœ… Downloaded 58000 files in 2641 seconds\n",
      "âœ… Downloaded 60000 files in 2731 seconds\n",
      "âœ… Downloaded 62000 files in 2818 seconds\n",
      "âœ… Downloaded 64000 files in 2906 seconds\n",
      "âœ… Downloaded 66000 files in 2997 seconds\n",
      "ðŸ“¥ Year 1993 complete: 9006/9006 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1994\n",
      "ðŸ” Found 8952 CSV files for 1994\n",
      "âœ… Downloaded 68000 files in 3095 seconds\n",
      "âœ… Downloaded 70000 files in 3187 seconds\n",
      "âœ… Downloaded 72000 files in 3275 seconds\n",
      "âœ… Downloaded 74000 files in 3361 seconds\n",
      "ðŸ“¥ Year 1994 complete: 8952/8952 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1995\n",
      "ðŸ” Found 8790 CSV files for 1995\n",
      "âœ… Downloaded 76000 files in 3453 seconds\n",
      "âœ… Downloaded 78000 files in 3542 seconds\n",
      "âœ… Downloaded 80000 files in 3628 seconds\n",
      "âœ… Downloaded 82000 files in 3710 seconds\n",
      "âœ… Downloaded 84000 files in 3797 seconds\n",
      "ðŸ“¥ Year 1995 complete: 8790/8790 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1996\n",
      "ðŸ” Found 8597 CSV files for 1996\n",
      "âœ… Downloaded 86000 files in 3895 seconds\n",
      "âœ… Downloaded 88000 files in 3984 seconds\n",
      "âœ… Downloaded 90000 files in 4072 seconds\n",
      "âœ… Downloaded 92000 files in 4160 seconds\n",
      "ðŸ“¥ Year 1996 complete: 8597/8597 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1997\n",
      "ðŸ” Found 8540 CSV files for 1997\n",
      "âœ… Downloaded 94000 files in 4254 seconds\n",
      "âœ… Downloaded 96000 files in 4346 seconds\n",
      "âœ… Downloaded 98000 files in 4433 seconds\n",
      "âœ… Downloaded 100000 files in 4525 seconds\n",
      "ðŸ“¥ Year 1997 complete: 8540/8540 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1998\n",
      "ðŸ” Found 8519 CSV files for 1998\n",
      "âœ… Downloaded 102000 files in 4620 seconds\n",
      "âœ… Downloaded 104000 files in 4712 seconds\n",
      "âœ… Downloaded 106000 files in 4800 seconds\n",
      "â³ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/1998/71914399999.csv â€“ retrying in 1 sec...\n",
      "âœ… Downloaded 108000 files in 4889 seconds\n",
      "âœ… Downloaded 110000 files in 4980 seconds\n",
      "ðŸ“¥ Year 1998 complete: 8519/8519 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 1999\n",
      "ðŸ” Found 8464 CSV files for 1999\n",
      "âœ… Downloaded 112000 files in 5076 seconds\n",
      "âœ… Downloaded 114000 files in 5169 seconds\n",
      "âœ… Downloaded 116000 files in 5255 seconds\n",
      "âœ… Downloaded 118000 files in 5344 seconds\n",
      "ðŸ“¥ Year 1999 complete: 8464/8464 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2000\n",
      "ðŸ” Found 8279 CSV files for 2000\n",
      "â³ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2000/03520099999.csv â€“ retrying in 1 sec...\n",
      "âœ… Downloaded 120000 files in 5442 seconds\n",
      "âœ… Downloaded 122000 files in 5534 seconds\n",
      "âœ… Downloaded 124000 files in 5622 seconds\n",
      "âœ… Downloaded 126000 files in 5717 seconds\n",
      "ðŸ“¥ Year 2000 complete: 8279/8279 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2001\n",
      "ðŸ” Found 9008 CSV files for 2001\n",
      "âœ… Downloaded 128000 files in 5813 seconds\n",
      "âœ… Downloaded 130000 files in 5903 seconds\n",
      "âœ… Downloaded 132000 files in 5992 seconds\n",
      "âœ… Downloaded 134000 files in 6082 seconds\n",
      "âœ… Downloaded 136000 files in 6170 seconds\n",
      "ðŸ“¥ Year 2001 complete: 9008/9008 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2002\n",
      "ðŸ” Found 8990 CSV files for 2002\n",
      "âœ… Downloaded 138000 files in 6267 seconds\n",
      "âœ… Downloaded 140000 files in 6360 seconds\n",
      "âœ… Downloaded 142000 files in 6454 seconds\n",
      "âœ… Downloaded 144000 files in 6549 seconds\n",
      "ðŸ“¥ Year 2002 complete: 8990/8990 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2003\n",
      "ðŸ” Found 9081 CSV files for 2003\n",
      "âœ… Downloaded 146000 files in 6645 seconds\n",
      "âœ… Downloaded 148000 files in 6736 seconds\n",
      "âœ… Downloaded 150000 files in 6823 seconds\n",
      "âœ… Downloaded 152000 files in 6917 seconds\n",
      "âœ… Downloaded 154000 files in 7005 seconds\n",
      "ðŸ“¥ Year 2003 complete: 9081/9081 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2004\n",
      "ðŸ” Found 9574 CSV files for 2004\n",
      "âœ… Downloaded 156000 files in 7104 seconds\n",
      "âœ… Downloaded 158000 files in 7197 seconds\n",
      "âœ… Downloaded 160000 files in 7285 seconds\n",
      "âœ… Downloaded 162000 files in 7377 seconds\n",
      "ðŸ“¥ Year 2004 complete: 9574/9574 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2005\n",
      "ðŸ” Found 10130 CSV files for 2005\n",
      "âœ… Downloaded 164000 files in 7478 seconds\n",
      "âœ… Downloaded 166000 files in 7569 seconds\n",
      "âœ… Downloaded 168000 files in 7657 seconds\n",
      "âœ… Downloaded 170000 files in 7749 seconds\n",
      "âœ… Downloaded 172000 files in 7843 seconds\n",
      "ðŸ“¥ Year 2005 complete: 10130/10130 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2006\n",
      "ðŸ” Found 9479 CSV files for 2006\n",
      "âœ… Downloaded 174000 files in 7945 seconds\n",
      "âœ… Downloaded 176000 files in 8042 seconds\n",
      "âœ… Downloaded 178000 files in 8139 seconds\n",
      "âœ… Downloaded 180000 files in 8228 seconds\n",
      "âœ… Downloaded 182000 files in 8322 seconds\n",
      "ðŸ“¥ Year 2006 complete: 9479/9479 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2007\n",
      "ðŸ” Found 9782 CSV files for 2007\n",
      "âœ… Downloaded 184000 files in 8419 seconds\n",
      "âœ… Downloaded 186000 files in 8508 seconds\n",
      "âœ… Downloaded 188000 files in 8599 seconds\n",
      "â³ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2007/71026899999.csv â€“ retrying in 1 sec...\n",
      "âœ… Downloaded 190000 files in 8688 seconds\n",
      "â³ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2007/72434503966.csv â€“ retrying in 1 sec...\n",
      "âœ… Downloaded 192000 files in 8780 seconds\n",
      "ðŸ“¥ Year 2007 complete: 9782/9782 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2008\n",
      "ðŸ” Found 10363 CSV files for 2008\n",
      "âœ… Downloaded 194000 files in 8880 seconds\n",
      "âœ… Downloaded 196000 files in 8973 seconds\n",
      "âœ… Downloaded 198000 files in 9067 seconds\n",
      "âœ… Downloaded 200000 files in 9162 seconds\n",
      "âœ… Downloaded 202000 files in 9257 seconds\n",
      "ðŸ“¥ Year 2008 complete: 10363/10363 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2009\n",
      "ðŸ” Found 10723 CSV files for 2009\n",
      "âœ… Downloaded 204000 files in 9361 seconds\n",
      "âœ… Downloaded 206000 files in 9455 seconds\n",
      "âœ… Downloaded 208000 files in 9549 seconds\n",
      "âœ… Downloaded 210000 files in 9641 seconds\n",
      "â³ 503 for https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2009/72635594871.csv â€“ retrying in 1 sec...\n",
      "âœ… Downloaded 212000 files in 9736 seconds\n",
      "âœ… Downloaded 214000 files in 9827 seconds\n",
      "ðŸ“¥ Year 2009 complete: 10723/10723 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2010\n",
      "ðŸ” Found 10902 CSV files for 2010\n",
      "âœ… Downloaded 216000 files in 9927 seconds\n",
      "âœ… Downloaded 218000 files in 10018 seconds\n",
      "âœ… Downloaded 220000 files in 10111 seconds\n",
      "âœ… Downloaded 222000 files in 10205 seconds\n",
      "âœ… Downloaded 224000 files in 10295 seconds\n",
      "ðŸ“¥ Year 2010 complete: 10902/10902 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2011\n",
      "ðŸ” Found 11088 CSV files for 2011\n",
      "âœ… Downloaded 226000 files in 10393 seconds\n",
      "âœ… Downloaded 228000 files in 10488 seconds\n",
      "âœ… Downloaded 230000 files in 10586 seconds\n",
      "âœ… Downloaded 232000 files in 10682 seconds\n",
      "âœ… Downloaded 234000 files in 10776 seconds\n",
      "âœ… Downloaded 236000 files in 10867 seconds\n",
      "ðŸ“¥ Year 2011 complete: 11088/11088 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2012\n",
      "ðŸ” Found 11875 CSV files for 2012\n",
      "âœ… Downloaded 238000 files in 10964 seconds\n",
      "âœ… Downloaded 240000 files in 11051 seconds\n",
      "âœ… Downloaded 242000 files in 11143 seconds\n",
      "âœ… Downloaded 244000 files in 11235 seconds\n",
      "âœ… Downloaded 246000 files in 11329 seconds\n",
      "âœ… Downloaded 248000 files in 11418 seconds\n",
      "ðŸ“¥ Year 2012 complete: 11875/11875 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2013\n",
      "ðŸ” Found 11870 CSV files for 2013\n",
      "âœ… Downloaded 250000 files in 11518 seconds\n",
      "âœ… Downloaded 252000 files in 11609 seconds\n",
      "âœ… Downloaded 254000 files in 11698 seconds\n",
      "âœ… Downloaded 256000 files in 11789 seconds\n",
      "âœ… Downloaded 258000 files in 11883 seconds\n",
      "ðŸ“¥ Year 2013 complete: 11870/11870 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2014\n",
      "ðŸ” Found 11958 CSV files for 2014\n",
      "âœ… Downloaded 260000 files in 11981 seconds\n",
      "âœ… Downloaded 262000 files in 12081 seconds\n",
      "âœ… Downloaded 264000 files in 12175 seconds\n",
      "âœ… Downloaded 266000 files in 12273 seconds\n",
      "âœ… Downloaded 268000 files in 12366 seconds\n",
      "âœ… Downloaded 270000 files in 12458 seconds\n",
      "ðŸ“¥ Year 2014 complete: 11958/11958 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2015\n",
      "ðŸ” Found 12100 CSV files for 2015\n",
      "âœ… Downloaded 272000 files in 12553 seconds\n",
      "âœ… Downloaded 274000 files in 12647 seconds\n",
      "âœ… Downloaded 276000 files in 12744 seconds\n",
      "âœ… Downloaded 278000 files in 12838 seconds\n",
      "âœ… Downloaded 280000 files in 12931 seconds\n",
      "âœ… Downloaded 282000 files in 13027 seconds\n",
      "ðŸ“¥ Year 2015 complete: 12100/12100 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2016\n",
      "ðŸ” Found 12111 CSV files for 2016\n",
      "âœ… Downloaded 284000 files in 13125 seconds\n",
      "âœ… Downloaded 286000 files in 13218 seconds\n",
      "âœ… Downloaded 288000 files in 13310 seconds\n",
      "âœ… Downloaded 290000 files in 13404 seconds\n",
      "âœ… Downloaded 292000 files in 13497 seconds\n",
      "âœ… Downloaded 294000 files in 13594 seconds\n",
      "âœ… Downloaded 296000 files in 13683 seconds\n",
      "ðŸ“¥ Year 2016 complete: 12111/12111 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2017\n",
      "ðŸ” Found 12337 CSV files for 2017\n",
      "âœ… Downloaded 298000 files in 13783 seconds\n",
      "âœ… Downloaded 300000 files in 13876 seconds\n",
      "âœ… Downloaded 302000 files in 13967 seconds\n",
      "âœ… Downloaded 304000 files in 14062 seconds\n",
      "âœ… Downloaded 306000 files in 14162 seconds\n",
      "âœ… Downloaded 308000 files in 14257 seconds\n",
      "ðŸ“¥ Year 2017 complete: 12337/12337 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2018\n",
      "ðŸ” Found 12426 CSV files for 2018\n",
      "âœ… Downloaded 310000 files in 14354 seconds\n",
      "âœ… Downloaded 312000 files in 14449 seconds\n",
      "âœ… Downloaded 314000 files in 14548 seconds\n",
      "âœ… Downloaded 316000 files in 14644 seconds\n",
      "âœ… Downloaded 318000 files in 14739 seconds\n",
      "âœ… Downloaded 320000 files in 14834 seconds\n",
      "ðŸ“¥ Year 2018 complete: 12426/12426 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2019\n",
      "ðŸ” Found 12387 CSV files for 2019\n",
      "âœ… Downloaded 322000 files in 14933 seconds\n",
      "âœ… Downloaded 324000 files in 15024 seconds\n",
      "âœ… Downloaded 326000 files in 15114 seconds\n",
      "âœ… Downloaded 328000 files in 15205 seconds\n",
      "âœ… Downloaded 330000 files in 15298 seconds\n",
      "âœ… Downloaded 332000 files in 15386 seconds\n",
      "ðŸ“¥ Year 2019 complete: 12387/12387 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2020\n",
      "ðŸ” Found 12299 CSV files for 2020\n",
      "âœ… Downloaded 334000 files in 15489 seconds\n",
      "âœ… Downloaded 336000 files in 15586 seconds\n",
      "âœ… Downloaded 338000 files in 15675 seconds\n",
      "âœ… Downloaded 340000 files in 15765 seconds\n",
      "âœ… Downloaded 342000 files in 15858 seconds\n",
      "âœ… Downloaded 344000 files in 15947 seconds\n",
      "ðŸ“¥ Year 2020 complete: 12299/12299 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2021\n",
      "ðŸ” Found 12275 CSV files for 2021\n",
      "âœ… Downloaded 346000 files in 16047 seconds\n",
      "âœ… Downloaded 348000 files in 16146 seconds\n",
      "âœ… Downloaded 350000 files in 16240 seconds\n",
      "âœ… Downloaded 352000 files in 16343 seconds\n",
      "âœ… Downloaded 354000 files in 16436 seconds\n",
      "âœ… Downloaded 356000 files in 16528 seconds\n",
      "ðŸ“¥ Year 2021 complete: 12275/12275 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2022\n",
      "ðŸ” Found 12319 CSV files for 2022\n",
      "âœ… Downloaded 358000 files in 16629 seconds\n",
      "âœ… Downloaded 360000 files in 16719 seconds\n",
      "âœ… Downloaded 362000 files in 16812 seconds\n",
      "âœ… Downloaded 364000 files in 16909 seconds\n",
      "âœ… Downloaded 366000 files in 17004 seconds\n",
      "âœ… Downloaded 368000 files in 17103 seconds\n",
      "âœ… Downloaded 370000 files in 17191 seconds\n",
      "ðŸ“¥ Year 2022 complete: 12319/12319 files downloaded\n",
      "\n",
      "ðŸ“… Processing year: 2023\n",
      "ðŸ” Found 12311 CSV files for 2023\n",
      "âœ… Downloaded 372000 files in 17287 seconds\n",
      "âœ… Downloaded 374000 files in 17375 seconds\n",
      "âœ… Downloaded 376000 files in 17463 seconds\n",
      "âœ… Downloaded 378000 files in 17550 seconds\n",
      "âœ… Downloaded 380000 files in 17641 seconds\n",
      "âœ… Downloaded 382000 files in 17736 seconds\n",
      "ðŸ“¥ Year 2023 complete: 12311/12311 files downloaded\n",
      "\n",
      "âœ… All done! Total downloaded: 382400 files in 17754 seconds.\n"
     ]
    }
   ],
   "source": [
    "## Script to download all NOAA weather data from 1970 - 2023\n",
    "\n",
    "\n",
    "NOAA_BASE_URL = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access\"\n",
    "BASE_PATH = \"/home/jovyan/Project/gsod\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "YEARS = list(range(1970, 2024))\n",
    "MAX_THREADS = 10  \n",
    "\n",
    "os.makedirs(BASE_PATH, exist_ok=True)\n",
    "\n",
    "def download_file(file_url, local_path, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            if not os.path.exists(local_path):\n",
    "                urllib.request.urlretrieve(file_url, local_path)\n",
    "                return True\n",
    "            else:\n",
    "                return False  # Already exists\n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.code == 503:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"â³ 503 for {file_url} â€“ retrying in {wait_time} sec...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"âŒ HTTP error for {file_url}: {e}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Other error for {file_url}: {e}\")\n",
    "            break\n",
    "    return False\n",
    "\n",
    "start_time = time.time()\n",
    "total_downloaded = 0\n",
    "failed_downloads = []\n",
    "\n",
    "try:\n",
    "    for year in YEARS:\n",
    "        print(f\"\\nðŸ“… Processing year: {year}\")\n",
    "        year_path = os.path.join(BASE_PATH, str(year))\n",
    "        os.makedirs(year_path, exist_ok=True)\n",
    "\n",
    "        url = f\"{NOAA_BASE_URL}/{year}/\"\n",
    "        req = urllib.request.Request(url, headers=HEADERS)\n",
    "\n",
    "        try:\n",
    "            with urllib.request.urlopen(req) as response:\n",
    "                html = response.read().decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to fetch directory listing for {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        csv_links = [link.get(\"href\") for link in soup.find_all(\"a\") if link.get(\"href\", \"\").endswith(\".csv\")]\n",
    "        print(f\"ðŸ” Found {len(csv_links)} CSV files for {year}\")\n",
    "\n",
    "        tasks = []\n",
    "        with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "            for filename in csv_links:\n",
    "                file_url = url + filename\n",
    "                local_file = os.path.join(year_path, filename)\n",
    "                tasks.append(executor.submit(download_file, file_url, local_file))\n",
    "\n",
    "            year_downloaded = 0\n",
    "            for future, filename in zip(as_completed(tasks), csv_links):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        total_downloaded += 1\n",
    "                        year_downloaded += 1\n",
    "                        if total_downloaded % 2000 == 0:\n",
    "                            elapsed = int(time.time() - start_time)\n",
    "                            print(f\"âœ… Downloaded {total_downloaded} files in {elapsed} seconds\")\n",
    "                except Exception as e:\n",
    "                    failed_downloads.append(url + filename)\n",
    "                    print(f\"âŒ Failed to download {filename}: {e}\")\n",
    "\n",
    "        print(f\"ðŸ“¥ Year {year} complete: {year_downloaded}/{len(csv_links)} files downloaded\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ï¸ Interrupted by user\")\n",
    "\n",
    "#  Save failed downloads\n",
    "if failed_downloads:\n",
    "    with open(\"failed_downloads.txt\", \"w\") as f:\n",
    "        for url in failed_downloads:\n",
    "            f.write(url + \"\\n\")\n",
    "    print(f\"âš ï¸ Logged {len(failed_downloads)} failed downloads to 'failed_downloads.txt'\")\n",
    "\n",
    "#  Done\n",
    "elapsed = int(time.time() - start_time)\n",
    "print(f\"\\nâœ… All done! Total downloaded: {total_downloaded} files in {elapsed} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c74c00-b3e7-4a8b-ab5f-afc66f35118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CSV folder size: 32.70 GB\n"
     ]
    }
   ],
   "source": [
    "## Verify size of download\n",
    "\n",
    "def get_folder_size(path):\n",
    "    total = 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if os.path.isfile(fp):\n",
    "                total += os.path.getsize(fp)\n",
    "    return total / (1024 ** 3)  \n",
    "\n",
    "size_gb = get_folder_size(\"/home/jovyan/Project/gsod\")\n",
    "print(f\"Original CSV folder size: {size_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c411e7-bce5-4f24-bdc3-5dbccd10ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GSOD CSV to Parquet\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.range(15).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2eb80c-0078-4d59-91f7-30009e1675af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: CSVs converted to Parquet\n"
     ]
    }
   ],
   "source": [
    "## Convert to Parquet\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import substring, lpad, concat_ws, col\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", DoubleType(), True),\n",
    "    StructField(\"LONGITUDE\", DoubleType(), True),\n",
    "    StructField(\"ELEVATION\", DoubleType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"TEMP\", DoubleType(), True),\n",
    "    StructField(\"DEWP\", DoubleType(), True),\n",
    "    StructField(\"SLP\", DoubleType(), True),\n",
    "    StructField(\"STP\", DoubleType(), True),\n",
    "    StructField(\"VISIB\", DoubleType(), True),\n",
    "    StructField(\"WDSP\", DoubleType(), True),\n",
    "    StructField(\"MXSPD\", DoubleType(), True),\n",
    "    StructField(\"GUST\", DoubleType(), True),\n",
    "    StructField(\"MAX\", StringType(), True),\n",
    "    StructField(\"MIN\", StringType(), True),\n",
    "    StructField(\"PRCP\", StringType(), True),\n",
    "    StructField(\"SNDP\", DoubleType(), True),\n",
    "    StructField(\"FRSHTT\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"/home/jovyan/Project/gsod\")\n",
    "\n",
    "df = df.withColumn(\"year\", substring(\"DATE\", 1, 4))\n",
    "\n",
    "#  Load station metadata\n",
    "meta = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"data/isd-history.csv\")\n",
    "\n",
    "# Create STATION ID to match GSOD format\n",
    "meta = meta.withColumn(\"STATION\", concat_ws(\"\",\n",
    "    lpad(col(\"USAF\").cast(\"string\"), 6, \"0\"),\n",
    "    lpad(col(\"WBAN\").cast(\"string\"), 5, \"0\")\n",
    ")).withColumnRenamed(\"ELEV(M)\", \"ELEV\") \\\n",
    "  .select(\"STATION\", \"CTRY\", \"STATE\", \"LAT\", \"LON\", \"ELEV\")\n",
    "\n",
    "\n",
    "#  Join metadata into GSOD DataFrame\n",
    "df_enriched = df.join(meta, on=\"STATION\", how=\"left\")\n",
    "\n",
    "# Write to Parquet (with metadata, partitioned by year)\n",
    "df_enriched.repartition(\"year\").write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .parquet(\"/home/jovyan/Project/gsod_parquet_enriched\")\n",
    "\n",
    "print(\"Done: CSVs converted to Parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab5b7ff-3eeb-4b54-af02-414f7dfcaf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: string (nullable = true)\n",
      " |-- MIN: string (nullable = true)\n",
      " |-- PRCP: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: string (nullable = true)\n",
      " |-- CTRY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      " |-- ELEV: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "+-----------+----------+---------+----------+---------+--------------------+----+----+----+----+------+----+-----+----+-----+---+-----+----+------+----+-----+------+-------+-----+----+\n",
      "|    STATION|      DATE| LATITUDE| LONGITUDE|ELEVATION|                NAME|TEMP|DEWP| SLP| STP| VISIB|WDSP|MXSPD|GUST|  MAX|MIN| PRCP|SNDP|FRSHTT|CTRY|STATE|   LAT|    LON| ELEV|year|\n",
      "+-----------+----------+---------+----------+---------+--------------------+----+----+----+----+------+----+-----+----+-----+---+-----+----+------+----+-----+------+-------+-----+----+\n",
      "|72401599999|2023-01-01|37.074194|-77.957528|    133.8|ALLEN C PERKINSON...|55.2|24.0|51.2|24.0|9999.9| 0.0|999.9| 0.0|  8.6| 24|  5.4|24.0|  11.1|  US|   VA|37.074|-77.958|133.8|2023|\n",
      "|23847099999|2023-10-13|     61.3|      71.3|     33.4|        SYTOMINO, RS|38.3| 8.0|36.4| 8.0|1000.4| 8.0|996.3| 8.0|  6.2|  8|  9.5| 8.0|  11.7|  RS| NULL|  61.3|   71.3| 33.4|2023|\n",
      "|72401599999|2023-01-02|37.074194|-77.957528|    133.8|ALLEN C PERKINSON...|53.7|24.0|48.5|24.0|9999.9| 0.0|999.9| 0.0|  8.8| 24|  3.7|24.0|   8.9|  US|   VA|37.074|-77.958|133.8|2023|\n",
      "|23847099999|2023-10-14|     61.3|      71.3|     33.4|        SYTOMINO, RS|41.1| 8.0|37.3| 8.0| 998.0| 8.0|994.0| 8.0|  6.2|  8|  9.7| 8.0|  13.6|  RS| NULL|  61.3|   71.3| 33.4|2023|\n",
      "|72401599999|2023-01-03|37.074194|-77.957528|    133.8|ALLEN C PERKINSON...|57.8|24.0|52.3|24.0|9999.9| 0.0|999.9| 0.0|  9.4| 24|  8.9|24.0|  15.0|  US|   VA|37.074|-77.958|133.8|2023|\n",
      "+-----------+----------+---------+----------+---------+--------------------+----+----+----+----+------+----+-----+----+-----+---+-----+----+------+----+-----+------+-------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_enriched = spark.read.parquet(\"/home/jovyan/Project/gsod_parquet_enriched\")\n",
    "df_enriched.printSchema()\n",
    "df_enriched.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d1eb5c-4c21-4520-a69d-334cfe7afeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_replace, when\n",
    "# 1. Drop rows with missing essential fields\n",
    "essential_cols = [\"TEMP\", \"LATITUDE\", \"LONGITUDE\"]\n",
    "df_clean = df.dropna(subset=essential_cols)\n",
    "\n",
    "# 2. Filter out placeholder or unrealistic values\n",
    "df_clean = df_clean.filter((col(\"TEMP\") > -1750) & (col(\"TEMP\") < 1750)) \\\n",
    "                   .filter((col(\"SLP\") < 1100) & (col(\"SLP\") > 800)) \\\n",
    "                   .filter(~col(\"PRCP\").isin([\"99.99\", \"999.9\", \"99.9\", \"999.0\"]))\n",
    "\n",
    "# 3. Remove duplicates\n",
    "df_clean = df_clean.dropDuplicates([\"STATION\", \"DATE\"])\n",
    "\n",
    "# 4. Clean special flags in MAX/MIN and PRCP columns\n",
    "df_clean = df_clean.withColumn(\"MAX\", regexp_replace(col(\"MAX\"), \"[*]\", \"\").cast(\"double\"))\n",
    "df_clean = df_clean.withColumn(\"MIN\", regexp_replace(col(\"MIN\"), \"[*]\", \"\").cast(\"double\"))\n",
    "\n",
    "# 5. Remove special flags from PRCP column\n",
    "df_clean = df_clean.withColumn(\"PRCP\", regexp_replace(col(\"PRCP\"), \"[A-Z]\", \"\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e19410-bd3f-4424-8a9b-1c74e943f9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761c049-8ac5-466c-995f-511ec800bbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
